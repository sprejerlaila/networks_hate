{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Combine profile data, botometer and m3 demographics into one users features dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total ids to collect\n",
    "ids1 = list(pd.read_csv('data/retweeters_users.csv').user_id.values.astype('str'))\n",
    "ids2 = list(pd.read_csv('data/retweeters_users_cc.csv').user_id.values.astype('str'))\n",
    "ids = set(ids1 + ids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Botometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('botometer/data')\n",
    "files.remove('collected_botometer_ids.csv')\n",
    "files.remove('error_botometer_ids.csv')\n",
    "files.remove('nonexistent_botometer_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonexistent = list(pd.read_csv('botometer/data/nonexistent_botometer_ids.csv', header=None)[0].values.astype('str'))\n",
    "errors = list(pd.read_csv('botometer/data/error_botometer_ids.csv', header=None)[0].values.astype('str'))\n",
    "\n",
    "botometers = []\n",
    "for file in files:\n",
    "    with open('botometer/data/{}'.format(file)) as json_data:\n",
    "        botometers += json_data.readlines()[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_botometer(data):\n",
    "    cap_english, cap_universal = [],[]\n",
    "    raw_astroturf, raw_fake_follower, raw_financial, raw_other, raw_overall, raw_self_declared, raw_spammer = [],[],[],[],[],[],[]\n",
    "    user_id, id_str, screen_name = [],[], []\n",
    "    for dat in data:\n",
    "        try:\n",
    "            botom = json.loads(dat)\n",
    "        except:\n",
    "            continue\n",
    "        if 'raw_scores' in botom:\n",
    "            cap_english.append(botom['cap']['english'])\n",
    "            cap_universal.append(botom['cap']['universal'])\n",
    "            raw_astroturf.append(botom['raw_scores']['universal']['astroturf'])\n",
    "            raw_fake_follower.append(botom['raw_scores']['universal']['fake_follower'])\n",
    "            raw_financial.append(botom['raw_scores']['universal']['financial'])\n",
    "            raw_other.append(botom['raw_scores']['universal']['other'])\n",
    "            raw_overall.append(botom['raw_scores']['universal']['overall'])\n",
    "            raw_self_declared.append(botom['raw_scores']['universal']['self_declared'])\n",
    "            raw_spammer.append(botom['raw_scores']['universal']['spammer'])\n",
    "\n",
    "            user_id.append(str(botom['user']['user_data']['id_str']))\n",
    "            id_str.append(\"id_\" + botom['user']['user_data']['id_str'])\n",
    "            screen_name.append(botom['user']['user_data']['screen_name'])\n",
    "                \n",
    "    return pd.DataFrame({\n",
    "        \"user_id\":user_id, 'id': id_str, \"screen_name\":screen_name, \"cap_english\":cap_english, \"cap_universal\":cap_universal,\n",
    "        \"astroturf\":raw_astroturf, \"fake_follower\":raw_fake_follower, \"financial\":raw_financial, \"other\":raw_other,\n",
    "        \"overall\":raw_overall, \"self_declared\":raw_self_declared, \"spammer\":raw_spammer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "botometer_df = process_botometer(botometers).drop_duplicates('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids to collect:  946835\n",
      "n botometer collected:  959255\n",
      "nonexistent user ids:  26498\n",
      "other errors user ids:  37937\n"
     ]
    }
   ],
   "source": [
    "print('ids to collect: ', len(ids))\n",
    "print('n botometer collected: ', len(botometer_df))\n",
    "print('nonexistent user ids: ', len(nonexistent))\n",
    "print('other errors user ids: ', len(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profiles_clean_1.json\n",
      "profiles_clean_0.json\n",
      "clean_retweeters_profiles_2043.json\n",
      "profiles_clean_7.json\n",
      "profiles_clean_6.json\n",
      "profiles_clean_5.json\n",
      "clean_retweeters_profiles_2041.json\n",
      "profiles_clean_8.json\n",
      "retweeters_profiles_2041.json_clean.json\n",
      "profiles_clean_4.json\n",
      "profiles_0.json\n",
      "profiles_clean_3.json\n",
      "profiles_clean_2.json\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('data/profiles/processed')\n",
    "files.remove('.DS_Store')\n",
    "profiles = []\n",
    "for f in files:\n",
    "    with open('data/profiles/processed/{}'.format(f)) as json_data:\n",
    "        print(f)\n",
    "        profiles += json_data.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_profiles(data):\n",
    "    ids, screen_name = [], []\n",
    "    followers_count, friends_count, statuses_count, favourites_count = [], [], [], []\n",
    "    location, geo, created_at = [], [], []\n",
    "    protected, verified = [], []\n",
    "    lang = []\n",
    "    \n",
    "    for d in data:\n",
    "        try:\n",
    "            prof = json.loads(d)\n",
    "        except:\n",
    "            continue\n",
    "        if \"errors\" in prof:\n",
    "            continue\n",
    "        ids.append(str(prof['id']))\n",
    "        screen_name.append(prof['screen_name'])\n",
    "        followers_count.append(prof['followers_count'])\n",
    "        friends_count.append(prof['friends_count'])\n",
    "        statuses_count.append(prof['statuses_count'])\n",
    "        favourites_count.append(prof['favourites_count'])\n",
    "        location.append(prof['location'])\n",
    "        created_at.append(prof['created_at'])\n",
    "        protected.append(prof['protected'])\n",
    "        verified.append(prof['verified'])\n",
    "        lang.append(prof['lang'])\n",
    "        \n",
    "    df = pd.DataFrame({\"user_id\": ids, \"screen_name\":screen_name, \"followers_count\":followers_count,\\\n",
    "                      \"friends_count\":friends_count, 'statuses_count': statuses_count, 'favourites_count': favourites_count, \\\n",
    "                      \"location\": location, \"created_at\": created_at, \"protected\": protected, \"verified\": verified,\\\n",
    "                      \"lang\": lang})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles_df = process_profiles(profiles).drop_duplicates('user_id'); len(profiles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3 demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3files = os.listdir('m3/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_m3(data):\n",
    "    ids = []\n",
    "    male = []\n",
    "    female = []\n",
    "    age_18 = []\n",
    "    age_19_29 =[]\n",
    "    age_30_39 = []\n",
    "    age_40 = []\n",
    "    non_org = []\n",
    "    is_org = []\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        ids.append(str(key))\n",
    "        male.append(value['gender']['male'])\n",
    "        female.append(value['gender']['female'])\n",
    "        age_18.append(value['age']['<=18'])\n",
    "        age_19_29.append(value['age']['19-29'])\n",
    "        age_30_39.append(value['age']['30-39'])\n",
    "        age_40.append(value['age']['>=40'])\n",
    "        non_org.append(value['org']['non-org'])\n",
    "        is_org.append(value['org']['is-org'])\n",
    "    \n",
    "    df = pd.DataFrame({\"user_id\": ids, \"male\":male, \"female\":female,\"age_18\":age_18, \"age_19_29\":age_19_29,\\\n",
    "                      \"age_30_39\":age_30_39, \"age_40\":age_40, \"non_org\":non_org, \"is_org\":is_org})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215173"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_dfs = []\n",
    "for m3file in m3files:\n",
    "    f = pickle.load( open( \"m3/processed/\"+m3file, \"rb\" ) )\n",
    "    m3_dfs.append(process_m3(f))\n",
    "m3_df = pd.concat(m3_dfs).drop_duplicates('user_id'); len(m3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959255"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = botometer_df.merge(profiles_df.drop('screen_name',axis=1), on = 'user_id', how='left'); len(all_df)\n",
    "all_df = all_df.merge(m3_df, on = 'user_id', how='left'); len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'id', 'screen_name', 'cap_english', 'cap_universal',\n",
       "       'astroturf', 'fake_follower', 'financial', 'other', 'overall',\n",
       "       'self_declared', 'spammer', 'followers_count', 'friends_count',\n",
       "       'statuses_count', 'favourites_count', 'location', 'created_at',\n",
       "       'protected', 'verified', 'lang', 'male', 'female', 'age_18',\n",
       "       'age_19_29', 'age_30_39', 'age_40', 'non_org', 'is_org'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv('all_users_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
